# Lung Disease Classification and Localization using Deep Learning  

## ğŸ“Œ Overview  
This project implements **deep learning models** to classify and localize lung diseases using **chest X-ray images**. It utilizes **ConvNeXt** for multi-label disease classification and **Swin Transformer** for disease localization, improving diagnostic accuracy in medical imaging.  

---

## ğŸ” Key Features  
- âœ… **Multi-label Classification**: ConvNeXt-based model achieving **80.11% AUC** on ChestX-ray14 dataset  
- âœ… **Disease Localization**: Swin Transformer-based localization achieving **79% IoU**  
- âœ… **Grad-CAM Visualization**: Helps interpret model predictions by highlighting important regions  

---

## ğŸ“‚ Datasets Used  
- **[ChestX-ray14](https://nihcc.app.box.com/v/ChestXray-NIHCC)** â€“ 112,120 chest X-rays for classification  
- **[MIMIC-CXR](https://physionet.org/content/mimic-cxr/2.0.0/)** â€“ 377,110 chest X-ray images for multi-task learning  
- **[VinDR-CXR](https://vindr.ai/datasets/vindr-cxr)** â€“ 18,000 expert-annotated X-ray images for localization  

---

## ğŸ— Model Architectures  

### ğŸ¥ **1ï¸âƒ£ Classification: ConvNeXt**  
![ConvNeXt Architecture](Images/ConvNext.png)
- A modern **CNN-based model** with residual connections and **layer normalization**  
- Trained on **ChestX-ray14** dataset to classify **14 lung diseases**  
- Achieved **80.11% AUC** on the test set  

### ğŸ¥ **2ï¸âƒ£ Localization: Swin Transformer**  
![Swin Transformer Architecture](Images/Swin_Transformer.png)
- A **hierarchical vision transformer** that captures spatial relationships  
- Trained on **VinDR-CXR** dataset for disease localization  
- Achieved **79% Intersection over Union (IoU)** in identifying diseased regions  

---

## ğŸš€ Results  

| Task           | Model            | Performance  |
|---------------|-----------------|-------------|
| Classification | ConvNeXt         | 80.11% AUC  |
| Localization  | Swin Transformer | 79% IoU     |

![Classification Result 1](Images/classification%20Results.png)
![Classification Result 2](Images/Classification%20results%202.png)

---

## ğŸ”§ Implementation Steps  

### 1ï¸âƒ£ **Data Collection and Preprocessing**  
- Download datasets: **ChestX-ray14, MIMIC-CXR, and VinDR-CXR**  
- Resize images (**224x224** for ConvNeXt, **1024x1024** for Swin Transformer)  
- Normalize pixel values between **0 and 1** for model training  
- Convert labels into **multi-hot encoding** (classification) and **bounding boxes** (localization)  
- Split into **training, validation, and test sets**  

---

### 2ï¸âƒ£ **Model Selection and Architecture**  
#### ğŸ”¹ **Classification: ConvNeXt**  
- Uses **CNN-based residual blocks** for **multi-label classification**  
- Key modifications:  
  âœ… **Global Average Pooling Layer** â€“ Reduces dimensionality  
  âœ… **Fully Connected Layer (ReLU activation)** â€“ Enhances feature learning  
  âœ… **Dropout Layer** â€“ Prevents overfitting  
  âœ… **Sigmoid Activation** â€“ Outputs probability for each disease  

#### ğŸ”¹ **Localization: Swin Transformer**  
- **Hierarchical transformer-based model** for spatial feature extraction  
- Implements **shifted window attention mechanism** for better context awareness  
- Trained within a **Mask R-CNN framework** for **bounding-box localization**  

---

### 3ï¸âƒ£ **Model Training and Hyperparameter Tuning**  
- **Optimizer**: **AdamW** for efficient weight updates  
- **Loss functions**:  
  ğŸ”¹ **BCEWithLogitsLoss** â€“ Classification  
  ğŸ”¹ **IoU Loss** â€“ Localization  
- **Training Hyperparameters**:  
  ğŸ”¹ **Batch size** = 32  
  ğŸ”¹ **Learning rate** = 0.0001  
  ğŸ”¹ **Epochs** = 20 (Classification), **50+** (Localization)  
- **Regularization Techniques**:  
  ğŸ”¹ **Dropout layers**  
  ğŸ”¹ **Data augmentation** (rotation, scaling, flipping)  

---

### 4ï¸âƒ£ **Model Evaluation and Performance Metrics**  
- **Classification (ConvNeXt)**: Evaluated using **AUC (Area Under the Curve)** â€“ **80.11% AUC**  
- **Localization (Swin Transformer)**: Evaluated using **Intersection over Union (IoU)** â€“ **79% IoU**  
---

## ğŸ“Š Performance Metrics  
- ğŸ“Œ **AUC (Area Under Curve)** â€“ Classification  
- ğŸ“Œ **IoU (Intersection over Union)** â€“ Localization  
- ğŸ“Œ **Grad-CAM** â€“ Class Activation Map for interpretability  

---

## ğŸ“ Future Improvements  
ğŸ”¹ Experiment with **multi-task learning** to improve classification and localization jointly  
ğŸ”¹ Apply **self-supervised learning (DINO)** for better feature extraction  
ğŸ”¹ Enhance **attention mechanisms** for more precise localization  

---

## ğŸ“œ References  
ğŸ“Œ **ConvNeXt**: [Paper](https://arxiv.org/abs/2201.03545)  
ğŸ“Œ **Swin Transformer**: [Paper](https://arxiv.org/abs/2103.14030)  

